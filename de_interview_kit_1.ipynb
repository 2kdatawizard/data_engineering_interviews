{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2kdatawizard/data_engineering_interviews/blob/main/de_interview_kit_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1"
      ],
      "metadata": {
        "id": "nRuI7mDyhrRF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUtu4316QSHL",
        "outputId": "b19628b1-8dee-43fb-c5fb-304c775639d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 4, 4, 4, 4]\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1\n",
        "def create_functions():\n",
        "    function_list = []\n",
        "    for i in range(5):\n",
        "        function_list.append(lambda: i)\n",
        "    return function_list\n",
        "\n",
        "functions = create_functions()\n",
        "results = [f() for f in functions]\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions\n",
        "1. What will be the output of this code?\n",
        "2. Why does it behave this way?\n",
        "3. How would you modify the code to make it\n",
        "4. produce the expected result of [0, 1, 2, 3, 4]?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mzfiHLNVf1K-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2"
      ],
      "metadata": {
        "id": "r6QB5A9MhvAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2\n",
        "def process_large_dataset(data_source, batch_size=1000):\n",
        "    results = []\n",
        "\n",
        "    for i in range(0, len(data_source), batch_size):\n",
        "        batch = data_source[i:i+batch_size]\n",
        "        processed = [item * 2 for item in batch]\n",
        "        results.extend(processed)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Usage\n",
        "large_data = list(range(10000000))  # 10 million items\n",
        "processed_data = process_large_dataset(large_data)"
      ],
      "metadata": {
        "id": "Ftxri3ucgG1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions\n",
        "\n",
        "1. What potential issue might occur when running this code with very large datasets?\n",
        "2. Rewrite the function to be memory-efficient using generators and the yield keyword.\n",
        "3. If the data_source is actually a generator itself (not a list), would your solution\n",
        "still work? Why or why not?\n",
        "4. Explain how your solution improves memory usage compared to the original code."
      ],
      "metadata": {
        "id": "Cz-kXaozgNyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "la0QEzvrg26k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark findspark"
      ],
      "metadata": {
        "id": "NMmyxPUUghGX",
        "outputId": "46f571da-cd68-4e89-a2a3-2c5dff317beb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3\n",
        "You are working with a large dataset containing user activity logs for an e-commerce website. The data is available as a PySpark DataFrame with the following schema:\n",
        "\n",
        "```\n",
        "root\n",
        " |-- user_id: string\n",
        " |-- session_id: string\n",
        " |-- timestamp: timestamp\n",
        " |-- page_id: string\n",
        " |-- action: string\n",
        " |-- product_id: string\n",
        " |-- category_id: string\n",
        " |-- price: double\n",
        " |-- quantity: integer\n",
        "\n",
        "```\n",
        "Each row represents a user action (view, add_to_cart, purchase, remove_from_cart) on a product page.\n",
        "\n",
        "## Write PySpark code to:\n",
        "\n",
        "1. Calculate the conversion rate (percentage of product views that resulted in purchases) for each product category, but only include users who had at least 3 sessions\n",
        "2. For each user, find the average time between viewing a product and adding it to cart\n",
        "3. Identify the top 5 products that are most frequently abandoned (added to cart but never purchased in the same session)\n",
        "4. Create a user engagement metric that combines total time spent, number of actions, and purchase value, then segment users into 'High', 'Medium', and 'Low' engagement groups\n",
        "5. Optimize your solution for performance, considering data skew, shuffle operations, and caching strategies\n",
        "\n",
        "## Expected Solution Components\n",
        "Your solution should include:\n",
        "\n",
        "1. Import statements and initialization of SparkSession\n",
        "2. Any necessary data preprocessing steps\n",
        "3. Implementation of all required analyses using PySpark DataFrame/SQL API\n",
        "4. Explanation of performance optimization choices\n",
        "5. Sample code to write results to the appropriate storage format\n",
        "\n",
        "## Example Data Pattern\n",
        "```\n",
        "user_id: \"u123\", session_id: \"s456\", timestamp: \"2023-09-15 14:32:21\",\n",
        "page_id: \"p789\", action: \"view\", product_id: \"prod123\", category_id: \"electronics\",\n",
        "price: 599.99, quantity: null\n",
        "```\n",
        "\n",
        "```\n",
        "user_id: \"u123\", session_id: \"s456\", timestamp: \"2023-09-15 14:35:42\",\n",
        "page_id: \"p789\", action: \"add_to_cart\", product_id: \"prod123\", category_id: \"electronics\",\n",
        "price: 599.99, quantity: 1\n",
        "```\n",
        "\n",
        "```\n",
        "user_id: \"u123\", session_id: \"s456\", timestamp: \"2023-09-15 14:45:12\",\n",
        "page_id: \"checkout\", action: \"purchase\", product_id: \"prod123\", category_id: \"electronics\",\n",
        "price: 599.99, quantity: 1\n",
        "```"
      ],
      "metadata": {
        "id": "Kd9fYUfLg4Y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# --- Step 3: Specify the CSV URL ---\n",
        "file_name = \"https://github.com/2kdatawizard/data_engineering_interviews/blob/c5dc17292cc86a83b99151e9fdb3f2506a91a1c2/data/ecommerce_data.csv\"\n",
        "df = pd.read_csv(file_name)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "lnmXC90rjQD9",
        "outputId": "445a473e-5eb4-43e0-c3bd-a803c8dafaa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 1 fields in line 42, saw 47\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-66c926e8e5ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# --- Step 3: Specify the CSV URL ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://github.com/2kdatawizard/data_engineering_interviews/blob/c5dc17292cc86a83b99151e9fdb3f2506a91a1c2/data/ecommerce_data.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 42, saw 47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-jnesT8XjPuu"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}