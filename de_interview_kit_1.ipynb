{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2kdatawizard/data_engineering_interviews/blob/main/de_interview_kit_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1"
      ],
      "metadata": {
        "id": "nRuI7mDyhrRF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUtu4316QSHL",
        "outputId": "b19628b1-8dee-43fb-c5fb-304c775639d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 4, 4, 4, 4]\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1\n",
        "def create_functions():\n",
        "    function_list = []\n",
        "    for i in range(5):\n",
        "        function_list.append(lambda: i)\n",
        "    return function_list\n",
        "\n",
        "functions = create_functions()\n",
        "results = [f() for f in functions]\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions\n",
        "1. What will be the output of this code?\n",
        "2. Why does it behave this way?\n",
        "3. How would you modify the code to make it\n",
        "4. produce the expected result of [0, 1, 2, 3, 4]?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mzfiHLNVf1K-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2"
      ],
      "metadata": {
        "id": "r6QB5A9MhvAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2\n",
        "def process_large_dataset(data_source, batch_size=1000):\n",
        "    results = []\n",
        "\n",
        "    for i in range(0, len(data_source), batch_size):\n",
        "        batch = data_source[i:i+batch_size]\n",
        "        processed = [item * 2 for item in batch]\n",
        "        results.extend(processed)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Usage\n",
        "large_data = list(range(10000000))  # 10 million items\n",
        "processed_data = process_large_dataset(large_data)"
      ],
      "metadata": {
        "id": "Ftxri3ucgG1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions\n",
        "\n",
        "1. What potential issue might occur when running this code with very large datasets?\n",
        "2. Rewrite the function to be memory-efficient using generators and the yield keyword.\n",
        "3. If the data_source is actually a generator itself (not a list), would your solution\n",
        "still work? Why or why not?\n",
        "4. Explain how your solution improves memory usage compared to the original code."
      ],
      "metadata": {
        "id": "Cz-kXaozgNyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "la0QEzvrg26k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3\n",
        "You are working with a large dataset containing user activity logs for an e-commerce website. The data is available as a PySpark DataFrame with the following schema:\n",
        "\n",
        "```\n",
        "root\n",
        " |-- user_id: string\n",
        " |-- session_id: string\n",
        " |-- timestamp: timestamp\n",
        " |-- page_id: string\n",
        " |-- action: string\n",
        " |-- product_id: string\n",
        " |-- category_id: string\n",
        " |-- price: double\n",
        " |-- quantity: integer\n",
        "\n",
        "```\n",
        "Each row represents a user action (view, add_to_cart, purchase, remove_from_cart) on a product page.\n",
        "\n",
        "## Write PySpark code to:\n",
        "\n",
        "1. Calculate the conversion rate (percentage of product views that resulted in purchases) for each product category, but only include users who had at least 3 sessions\n",
        "2. For each user, find the average time between viewing a product and adding it to cart\n",
        "3. Identify the top 5 products that are most frequently abandoned (added to cart but never purchased in the same session)\n",
        "4. Create a user engagement metric that combines total time spent, number of actions, and purchase value, then segment users into 'High', 'Medium', and 'Low' engagement groups\n",
        "5. Optimize your solution for performance, considering data skew, shuffle operations, and caching strategies\n",
        "\n",
        "## Expected Solution Components\n",
        "Your solution should include:\n",
        "\n",
        "1. Import statements and initialization of SparkSession\n",
        "2. Any necessary data preprocessing steps\n",
        "3. Implementation of all required analyses using PySpark DataFrame/SQL API\n",
        "4. Explanation of performance optimization choices\n",
        "5. Sample code to write results to the appropriate storage format\n",
        "\n",
        "## Example Data Pattern\n",
        "```\n",
        "user_id: \"u123\", session_id: \"s456\", timestamp: \"2023-09-15 14:32:21\",\n",
        "page_id: \"p789\", action: \"view\", product_id: \"prod123\", category_id: \"electronics\",\n",
        "price: 599.99, quantity: null\n",
        "```\n",
        "\n",
        "```\n",
        "user_id: \"u123\", session_id: \"s456\", timestamp: \"2023-09-15 14:35:42\",\n",
        "page_id: \"p789\", action: \"add_to_cart\", product_id: \"prod123\", category_id: \"electronics\",\n",
        "price: 599.99, quantity: 1\n",
        "```\n",
        "\n",
        "```\n",
        "user_id: \"u123\", session_id: \"s456\", timestamp: \"2023-09-15 14:45:12\",\n",
        "page_id: \"checkout\", action: \"purchase\", product_id: \"prod123\", category_id: \"electronics\",\n",
        "price: 599.99, quantity: 1\n",
        "```"
      ],
      "metadata": {
        "id": "Kd9fYUfLg4Y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install PySpark and Java in Colab\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark==3.2.0 findspark"
      ],
      "metadata": {
        "id": "NMmyxPUUghGX",
        "outputId": "f8b38eff-ccd8-4637-c435-139720e30ebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connecting to security.ub\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to r2u.stat.illinois\r                                                                               \rHit:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r                                                                               \rHit:4 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Collecting pyspark==3.2.0\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.11/dist-packages (2.0.1)\n",
            "Collecting py4j==0.10.9.2 (from pyspark==3.2.0)\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805895 sha256=857c3852df224dce81916ef78038cd4f6b0fe781fa22d516d6cea185dc3cfe71\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/91/e5/3ce3524bd3ec0206493bd1e371d8c2365b3186de9e542af601\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.1\n",
            "    Uninstalling pyspark-3.5.1:\n",
            "      Successfully uninstalled pyspark-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.7.2 requires pyspark[connect]>=3.5, but you have pyspark 3.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "py4j",
                  "pyspark"
                ]
              },
              "id": "582401625dfe4be496a9a65e0620ce53"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Configure environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "# Step 3: Import necessary libraries\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType, IntegerType\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "lnmXC90rjQD9",
        "outputId": "4310e8c7-3b8a-4124-b8f4-9a91264a0075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrameReader' object has no attribute 'cache'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-4c961253e88c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Cache the DataFrame since we'll be performing multiple analyses on it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# 1. Calculate conversion rate for each product category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameReader' object has no attribute 'cache'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "-jnesT8XjPuu"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}